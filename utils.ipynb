{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56637038-c6e5-456a-b189-88b3784636a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import os\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb3c15c-75b3-4935-bde6-252ffc8f458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54e1ff0-17e5-476c-aaf5-4add1a3a385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing and Lemmatization\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('[^a-zA-Z ]+', ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    text = \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    "    doc = nlp(text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c257024-7d87-416a-976a-c0bace9e67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_glove(destination_folder='glove', dim=100):\n",
    "    \"\"\"\n",
    "    Downloads and extracts GloVe embeddings in a Colab-compatible way.\n",
    "    Returns the path to the specific .txt file (e.g., glove.6B.100d.txt).\n",
    "    \"\"\"\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    zip_path = os.path.join(destination_folder, 'glove.6B.zip')\n",
    "    glove_file = os.path.join(destination_folder, f'glove.6B.{dim}d.txt')\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(glove_file):\n",
    "        print(f\"GloVe file already exists at: {glove_file}\")\n",
    "        return glove_file\n",
    "\n",
    "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for data in tqdm(response.iter_content(1024), total=total_size // 1024, unit='KB'):\n",
    "            f.write(data)\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)\n",
    "\n",
    "    if os.path.exists(glove_file):\n",
    "        print(f\"GloVe downloaded and available at: {glove_file}\")\n",
    "        return glove_file\n",
    "    else:\n",
    "        raise FileNotFoundError(\"GloVe file was not found after extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7ca892-5efd-41b5-ba93-bc31cd3e03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe Embeddings\n",
    "def load_glove_embeddings(glove_file_path='glove.6B.100d.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90116ded-83bb-44f1-ae68-674213584760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to vector\n",
    "def document_vector(text, embeddings_index, dim=100):\n",
    "    words = text.split()\n",
    "    vectors = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "    if not vectors:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4945b90-4e8d-43a0-81bc-a65d1a2f88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load helper functions\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0d7a61-b317-463e-95c9-54e6f611258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for pipelines\n",
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, embeddings_index, dim=100):\n",
    "        self.embeddings_index = embeddings_index\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.vstack([document_vector(text, self.embeddings_index, self.dim) for text in X])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
