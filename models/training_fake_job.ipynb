{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e06e80a2-27ee-4efc-991f-bacf85f0c1c9",
    "outputId": "94059911-1ab8-4213-c45b-dc73f627e141"
   },
   "outputs": [],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubmmddCMOYfJ",
    "outputId": "d61e7c46-acca-43a1-e3e3-bb91b0dbe058"
   },
   "outputs": [],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "9S05F2r7BMKG",
    "outputId": "5e741181-f42e-4cae-d2ef-acb2c3ab2823"
   },
   "outputs": [],
   "source": [
    "pip install numpy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c84aead2-ea66-418b-96ca-705b852e8e9e",
    "outputId": "2224c3ac-6cba-4656-c54d-98e8625c3cab"
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import text_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi, Repository, hf_hub_download\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "ZmkusrQLgSZb"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, login\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def push_to_hub(model, embeddings_index, repo_name=\"ali9999/fake-job-detection-model\", hf_token=None):\n",
    "    \"\"\"\n",
    "    Push the trained model and GloVe embeddings index to Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    if hf_token:\n",
    "        login(token=hf_token)  # Optional: for explicit login\n",
    "\n",
    "    model_path = \"fraud_model.pt\"\n",
    "    embeddings_path = \"embeddings_index.pkl\"\n",
    "    repo_dir = \"hf_model_repo\"\n",
    "\n",
    "    # Save locally\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    with open(embeddings_path, \"wb\") as f:\n",
    "        pickle.dump(embeddings_index, f)\n",
    "\n",
    "    # Clone or initialize repo\n",
    "    if os.path.exists(repo_dir):\n",
    "        os.system(f\"rm -rf {repo_dir}\")  # optional: clean before re-cloning\n",
    "\n",
    "    repo = Repository(local_dir=repo_dir, clone_from=repo_name)\n",
    "\n",
    "    # Move files to repo directory\n",
    "    os.replace(model_path, os.path.join(repo_dir, model_path))\n",
    "    os.replace(embeddings_path, os.path.join(repo_dir, embeddings_path))\n",
    "\n",
    "    os.system('git config --global user.email \"sayyidaliahmed1@gmail.com\"')\n",
    "    os.system('git config --global user.name \"Ali Ahmed\"')\n",
    "\n",
    "    # Push\n",
    "    repo.git_add()\n",
    "    repo.git_commit(\"Add model and embeddings index\")\n",
    "    repo.git_push()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "036abc3b9de84416a19deca7fb8530c6",
      "4d93ef631a9940c798fe09212a0e3dfe",
      "5f017c287fe2489aa4c32f04ab7e5d41",
      "f2dc617bcc3b41ea848d0a7b138170ec",
      "4b128ec3add74c6195b34890b19a4941",
      "056ab70f86a34cc7b193dc86658a6a65",
      "7931021ba10842a4a912749ce5ebe5e3",
      "b367199161df431082c907ac8ebea5e5",
      "e7213acab9f648bf930ce575670912b5",
      "cce26d5fea4947da8e40ed20d3346b21",
      "c90c812bcf154eb3a9e56a9a388e3409",
      "3525ff99dd1b40838928c25b4137723d",
      "6a7007c825e54c1b9283f5e16fb78fb0",
      "9e3a0afa6a78404ea21d4dcb9914a3f5",
      "8b375555f8f4474097025d16ffdcf20d",
      "58e4532d437c499895ef3f0829f72fcc",
      "77dc314a18d84326a6573cccdfc852f8",
      "4dc2b903d8d74ffc808f86256a467ea9",
      "1ae23d044c4949868528102a09fd9beb",
      "c0e67316d9944b058a0e85d8be60b99f"
     ]
    },
    "id": "uuene5WTg6kC",
    "outputId": "46597d08-4b7d-4265-f3b7-5112d81f4377"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "fb748ac6-3821-4fdd-8ae8-004e5926791a"
   },
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim=300, embed_dim=16, num_classes=2):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, embed_dim),\n",
    "        )\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.embedding(x)\n",
    "        logits = self.classifier(z)\n",
    "        return z, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "734315fa-ae06-4cca-8acd-e1513dd5ae82"
   },
   "outputs": [],
   "source": [
    "def custom_loss(embeddings, logits, labels, k=3, alpha=0.6, margin=1.0, beta=0.9):\n",
    "    # Balanced cross-entropy\n",
    "    class_counts = torch.bincount(labels)\n",
    "    weights = 1.0 / (class_counts.float() + 1e-8)\n",
    "    weights = weights.to(logits.device)\n",
    "    classification_loss = nn.functional.cross_entropy(logits, labels, weight=weights)\n",
    "\n",
    "    embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "    contrastive_loss = 0.0\n",
    "    loss_count = 0\n",
    "\n",
    "    for cls in torch.unique(labels):\n",
    "        same_mask = labels == cls\n",
    "        other_mask = labels != cls\n",
    "        cluster = embeddings[same_mask]\n",
    "        other = embeddings[other_mask]\n",
    "\n",
    "        if cluster.size(0) < k + 1 or other.size(0) < k:\n",
    "            continue\n",
    "\n",
    "        # Dynamically computed centroid (differentiable)\n",
    "        centroid = cluster.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Pull loss (within class)\n",
    "        dists_within = torch.norm(cluster - centroid, dim=1)\n",
    "        farthest = cluster[torch.topk(dists_within, k).indices]\n",
    "        pull_loss = torch.sum((farthest - centroid) ** 2)\n",
    "\n",
    "        # Push loss (between class)\n",
    "        dists_other = torch.norm(other - centroid, dim=1)\n",
    "        topk_closest = other[torch.topk(-dists_other, k).indices]\n",
    "        push_dists = torch.norm(topk_closest - centroid.squeeze(), dim=1)\n",
    "        push_loss = torch.sum(torch.clamp(margin - push_dists, min=0) ** 2)\n",
    "\n",
    "        # Balanced contrastive loss\n",
    "        contrastive_loss += (1 - alpha) * pull_loss + alpha * push_loss\n",
    "        loss_count += 1\n",
    "\n",
    "    contrastive_loss = contrastive_loss / max(loss_count, 1)\n",
    "    return beta * classification_loss + (1 - beta) * contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "3Mi-C6cCCBjk"
   },
   "outputs": [],
   "source": [
    "def document_vector(text, word2vec_model, dim=300):\n",
    "    \"\"\"Compute the average Word2Vec vector for a document.\"\"\"\n",
    "    words = text.split()  # Simple tokenization; adjust based on your preprocessing\n",
    "    vectors = [word2vec_model[word] for word in words if word in word2vec_model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)  # Return zero vector if no valid words\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "83739939-a459-4ebd-9bd9-3fb684a743b0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_centroids(embeddings, labels, epoch):\n",
    "    emb_np = embeddings.detach().cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    # Compute centroids for each class\n",
    "    centroids = []\n",
    "    for cls in torch.unique(labels):\n",
    "        mask = labels == cls\n",
    "        class_embs = emb_np[mask.cpu().numpy()]\n",
    "        centroid = class_embs.mean(axis=0)\n",
    "        centroids.append((cls.item(), centroid))\n",
    "\n",
    "    # Plot first 2 dims + centroids\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(emb_np[:, 0], emb_np[:, 1], c=labels_np, cmap='coolwarm', alpha=0.5, s=10)\n",
    "    for cls, centroid in centroids:\n",
    "        plt.scatter(centroid[0], centroid[1], label=f\"Centroid Class {cls}\", s=120, marker='X', edgecolors='black')\n",
    "    plt.title(f\"Class Centroids - Epoch {epoch}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"centroid_plot_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def train():\n",
    "    # Load pre-cleaned dataset\n",
    "    df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "    X_texts = df['text'].tolist()\n",
    "    y = df['fraudulent'].values\n",
    "\n",
    "    # Load GloVe vectors\n",
    "    #glove_txt_path = text_utils.download_glove(destination_folder='glove', dim=300)\n",
    "    #embeddings_index = text_utils.load_glove_embeddings(glove_txt_path)\n",
    "        # Load or download Word2Vec model\n",
    "\n",
    "    model_name = \"word2vec-google-news-300\"\n",
    "    cache_path = os.path.join(os.path.expanduser(\"~\"), \"gensim-data\", model_name, f\"{model_name}.gz\")\n",
    "    print(\"Checking for cached Word2Vec model...\")\n",
    "    try:\n",
    "        # Check if model is already cached\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Loading cached Word2Vec model from {cache_path}...\")\n",
    "            word2vec_model = KeyedVectors.load_word2vec_format(cache_path, binary=True)\n",
    "            print(\"Cached Word2Vec model loaded.\")\n",
    "        else:\n",
    "            print(f\"Downloading {model_name}...\")\n",
    "            word2vec_model = api.load(model_name)\n",
    "            print(\"Word2Vec model downloaded and loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Word2Vec model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    #X_vectors = np.array([text_utils.document_vector(text, embeddings_index) for text in X_texts])\n",
    "\n",
    "    X_vectors = np.array([document_vector(text, word2vec_model, dim=300) for text in X_texts])\n",
    "\n",
    "    # Handle class imbalance\n",
    "    adasyn = ADASYN(sampling_strategy=0.4, random_state=42, n_neighbors=5)\n",
    "    X_res, y_res = adasyn.fit_resample(X_vectors, y)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Initialize model\n",
    "    model = Embedder()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(3000):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings, logits = model(X_train_tensor)\n",
    "        loss = custom_loss(embeddings, logits, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Plot centroids every 500 epochs\n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f\"Plotting centroids at epoch {epoch+1}\")\n",
    "            plot_centroids(embeddings, y_train_tensor, epoch + 1)\n",
    "\n",
    "    # Save model and embeddings\n",
    "    #torch.save(model.state_dict(), \"fraud_model.pt\")\n",
    "    #with open(\"embeddings_index.pkl\", \"wb\") as f:\n",
    "        #pickle.dump(embeddings_index, f)\n",
    "\n",
    "    #torch.save(model.state_dict(), \"fraud_model.pt\")\n",
    "    #with open(\"word2vec_model.pkl\", \"wb\") as f:\n",
    "        #word2vec_model.save(f)\n",
    "\n",
    "\n",
    "    print(\"Training done and saved locally.\")\n",
    "    #return model, X_test_tensor, y_test_tensor, embeddings_index\n",
    "\n",
    "    return model, X_test_tensor, y_test_tensor, word2vec_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3429f7c1-0cd3-40fd-a95a-fb624b57999e",
    "outputId": "bf5e1078-c505-4741-8df5-c509c75558b7"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, X_test_tensor, y_test_np, embeddings_index = train()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb_test, logits_test = model(X_test_tensor)\n",
    "\n",
    "    # Predictions\n",
    "    preds = torch.argmax(logits_test, dim=1).numpy()\n",
    "\n",
    "    #push_to_hub(model, embeddings_index, repo_name=\"ali9999/fake-job-detection-model\")\n",
    "    #print(\"Pushed model and embeddings to Hugging Face Hub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "43e7169d-95cd-4152-a08b-0853f5a0186a",
    "outputId": "4d79dbf4-f9ac-4f70-8178-8c5542d7e753"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test_np, preds)\n",
    "f1 = f1_score(y_test_np, preds)\n",
    "print(f\"\\nAccuracy: {acc:.3f}, F1 Score: {f1:.3f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_np, preds))\n",
    "# Clustering metrics\n",
    "emb_np = emb_test.numpy()\n",
    "sil_score = silhouette_score(emb_np, y_test_np)\n",
    "ari = adjusted_rand_score(y_test_np, preds)\n",
    "print(f\"\\nSilhouette Score: {sil_score:.3f}\")\n",
    "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(emb_np[:, 0], emb_np[:, 1], c=preds, cmap='coolwarm', alpha=0.7)\n",
    "plt.title(\"2D Embeddings - True Labels\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# These use true labels vs predicted clusters\n",
    "nmi = normalized_mutual_info_score(y_test_np, preds)\n",
    "hom = homogeneity_score(y_test_np, preds)\n",
    "comp = completeness_score(y_test_np, preds)\n",
    "v_score = v_measure_score(y_test_np, preds)\n",
    "\n",
    "# These use embeddings only\n",
    "ch_score = calinski_harabasz_score(emb_np, y_test_np)\n",
    "db_score = davies_bouldin_score(emb_np, y_test_np)\n",
    "\n",
    "print(f\"\\nNormalized Mutual Information: {nmi:.3f}\")\n",
    "print(f\"Homogeneity Score: {hom:.3f}\")\n",
    "print(f\"Completeness Score: {comp:.3f}\")\n",
    "print(f\"V-Measure: {v_score:.3f}\")\n",
    "print(f\"Calinski-Harabasz Score: {ch_score:.3f}\")\n",
    "print(f\"Davies-Bouldin Score: {db_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "Y7_CMeOVajL4",
    "outputId": "13bf5f28-76f7-4d12-f31b-fc2d338fbc40"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "emb_tsne = tsne.fit_transform(emb_np)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(emb_tsne[:, 0], emb_tsne[:, 1], c=preds, cmap='coolwarm', alpha=0.7)\n",
    "plt.title(\"t-SNE Visualization of Embeddings\")\n",
    "plt.colorbar(label=\"True Class\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "QbSMvZdEuA1W",
    "outputId": "dbc056f3-033c-40f3-cb4d-8ec64f214c77"
   },
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "emb_umap = reducer.fit_transform(emb_np)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(emb_umap[:, 0], emb_umap[:, 1], c=preds, cmap='coolwarm', alpha=0.7)\n",
    "plt.title(\"UMAP Visualization of Embeddings\")\n",
    "plt.colorbar(label=\"True Class\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "aUILeMSOudna",
    "outputId": "a74c17c8-41cb-4019-86ae-850b8337a890"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class0_mask = preds == 0\n",
    "class1_mask = preds == 1\n",
    "\n",
    "centroid_0 = emb_np[class0_mask].mean(axis=0)\n",
    "centroid_1 = emb_np[class1_mask].mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(emb_np[:, 0], emb_np[:, 1], c=preds, cmap='coolwarm', alpha=0.6)\n",
    "plt.scatter(*centroid_0[:2], c='blue', s=100, marker='x', label='Centroid Class 0')\n",
    "plt.scatter(*centroid_1[:2], c='red', s=100, marker='x', label='Centroid Class 1')\n",
    "plt.title(\"Embeddings with Centroids\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "eRepF96HuxXs",
    "outputId": "619fe82b-67a7-406a-83eb-10286c46aa42"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(x=emb_tsne[:, 0], y=emb_tsne[:, 1], color=preds.astype(str), title=\"Interactive t-SNE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FwRib1cu_Pt",
    "outputId": "96c92852-7339-4f00-e471-b5e8b32f711a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris, load_wine, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ========== Embedder Model ==========\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim=20, embed_dim=16, num_classes=3):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embed_dim),\n",
    "        )\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.embedding(x)\n",
    "        logits = self.classifier(z)\n",
    "        return z, logits\n",
    "\n",
    "# ========== Custom Loss ==========\n",
    "def custom_loss(embeddings, logits, labels, k=3, alpha=1.0, margin=1.0, beta=0.7):\n",
    "    class_counts = torch.bincount(labels)\n",
    "    weights = 1.0 / (class_counts.float() + 1e-8)\n",
    "    weights = weights.to(logits.device)\n",
    "    classification_loss = nn.functional.cross_entropy(logits, labels, weight=weights)\n",
    "\n",
    "    embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "    contrastive_loss = 0.0\n",
    "    loss_count = 0\n",
    "\n",
    "    for cls in torch.unique(labels):\n",
    "        same_mask = labels == cls\n",
    "        other_mask = labels != cls\n",
    "        cluster = embeddings[same_mask]\n",
    "        other = embeddings[other_mask]\n",
    "\n",
    "        if cluster.size(0) < k + 1 or other.size(0) < k:\n",
    "            continue\n",
    "\n",
    "        centroid = cluster.mean(dim=0, keepdim=True)\n",
    "        dists_within = torch.norm(cluster - centroid, dim=1)\n",
    "        farthest = cluster[torch.topk(dists_within, k).indices]\n",
    "        pull_loss = torch.sum((farthest - centroid) ** 2)\n",
    "\n",
    "        dists_other = torch.norm(other - centroid, dim=1)\n",
    "        topk_closest = other[torch.topk(-dists_other, k).indices]\n",
    "        push_dists = torch.norm(topk_closest - centroid.squeeze(), dim=1)\n",
    "        push_loss = torch.sum(torch.clamp(margin - push_dists, min=0) ** 2)\n",
    "\n",
    "        contrastive_loss += (1 - alpha) * pull_loss + alpha * push_loss\n",
    "        loss_count += 1\n",
    "\n",
    "    contrastive_loss = contrastive_loss / max(loss_count, 1)\n",
    "    return beta * classification_loss + (1 - beta) * contrastive_loss\n",
    "\n",
    "# ========== Visualizations ==========\n",
    "def plot_tsne(embeddings, labels, name=\"tsne_plot.png\"):\n",
    "    n_samples = embeddings.shape[0]\n",
    "    perplexity = min(30, max(5, n_samples // 3))\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    emb_2d = tsne.fit_transform(embeddings.detach().cpu().numpy())\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels.cpu().numpy(), cmap='tab10', alpha=0.6)\n",
    "    plt.title(\"t-SNE of Embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_centroids(embeddings, labels, epoch, save_dir=\"centroid_plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    emb_np = embeddings.detach().cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    centroids = []\n",
    "    for cls in np.unique(labels_np):\n",
    "        mask = labels_np == cls\n",
    "        centroid = emb_np[mask].mean(axis=0)\n",
    "        centroids.append((cls, centroid))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(emb_np[:, 0], emb_np[:, 1], c=labels_np, cmap='tab10', alpha=0.5)\n",
    "    for cls, c in centroids:\n",
    "        plt.scatter(c[0], c[1], s=100, marker='X', label=f\"Centroid {cls}\", edgecolors='black')\n",
    "    plt.title(f\"Centroids at Epoch {epoch}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{save_dir}/centroids_epoch_{epoch}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ========== Training ==========\n",
    "def train_on_dataset(X, y, dataset_name, num_classes):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    model = Embedder(input_dim=X.shape[1], num_classes=num_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        embeddings, logits = model(X_train_tensor)\n",
    "        loss = custom_loss(embeddings, logits, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"[{dataset_name}] Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                emb_test, logits_test = model(X_test_tensor)\n",
    "                pred_test = torch.argmax(logits_test, dim=1)\n",
    "                acc = accuracy_score(y_test_tensor.cpu(), pred_test.cpu())\n",
    "                print(f\"[{dataset_name}] Test Accuracy: {acc:.4f}\")\n",
    "                plot_tsne(emb_test, y_test_tensor, name=f\"visuals/{dataset_name}_tsne_epoch_{epoch}.png\")\n",
    "                plot_centroids(emb_test, y_test_tensor, epoch, save_dir=f\"visuals/{dataset_name}_centroids\")\n",
    "\n",
    "# ========== Run on Real Datasets ==========\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"visuals\", exist_ok=True)\n",
    "\n",
    "    datasets = {\n",
    "        \"iris\": (load_iris(return_X_y=True), 3),\n",
    "        \"wine\": (load_wine(return_X_y=True), 3),\n",
    "        \"digits\": (load_digits(return_X_y=True), 10),\n",
    "    }\n",
    "\n",
    "    for name, ((X, y), num_classes) in datasets.items():\n",
    "        print(f\"\\n--- Training on {name} dataset ---\")\n",
    "        train_on_dataset(X, y, dataset_name=name, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "3NAWeItu6dqE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
